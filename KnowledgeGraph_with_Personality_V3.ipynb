{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Yu1MNwuX28"
      },
      "source": [
        "# Knowledge Graph Construction with Personality Modeling\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Extract a knowledge graph from unstructured text\n",
        "2. Enrich the graph with personality traits of subjects\n",
        "3. Visualize and query the knowledge graph\n",
        "\n",
        "We use LangChain with Google's Gemini API as the LLM backbone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOilnAp1uX2_"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, let's install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xiWSIlpZuX3A",
        "outputId": "1cd32223-13aa-4e43-93ff-42efcac4aac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-3.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting neo4j\n",
            "  Downloading neo4j-6.0.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Collecting pyvis\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.37)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting google-ai-generativelanguage<1,>=0.7 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting filetype<2,>=1.2 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting langchain-community<0.4.0,>=0.3.0 (from langchain-experimental)\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from neo4j) (2025.2)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from pyvis) (7.34.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.12/dist-packages (from pyvis) (3.1.6)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from pyvis) (4.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.26.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.75.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.9.6->pyvis) (3.0.3)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.13.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis) (0.2.14)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.6.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading neo4j-6.0.2-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.8/325.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: filetype, requests, neo4j, mypy-extensions, marshmallow, jedi, typing-inspect, pyvis, dataclasses-json, google-ai-generativelanguage, langchain-google-genai, langchain-community, langchain-experimental\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.9.0 jedi-0.19.2 langchain-community-0.3.31 langchain-experimental-0.3.4 langchain-google-genai-2.1.12 marshmallow-3.26.1 mypy-extensions-1.1.0 neo4j-6.0.2 pyvis-0.3.2 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "d8c03dd5fc924d428d6c19d7ed2ba0c7",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install langchain langchain-google-genai langchain-experimental neo4j networkx pyvis python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4mjLXxruX3A"
      },
      "source": [
        "Import necessary libraries and set up environment variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7XbwdZAuX3B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import networkx as nx\n",
        "from dotenv import load_dotenv\n",
        "from pyvis.network import Network\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from langchain.graphs import NetworkxEntityGraph\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set Google API key\n",
        "GOOGLE_API_KEY = \"xxx\" # replace with your own"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkJLEpKPuX3C"
      },
      "source": [
        "Initialize the Gemini model via LangChain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5A47jTT5uX3D"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", google_api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jHlegcluX3E"
      },
      "source": [
        "## 2. Document Preprocessing\n",
        "\n",
        "Create functions to load and preprocess text documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Pmc0PmNOuX3F"
      },
      "outputs": [],
      "source": [
        "def load_document(file_path):\n",
        "    \"\"\"Load document from file\"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text preprocessing to remove unwanted characters\"\"\"\n",
        "    # Remove extra whitespace\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=4000, chunk_overlap=200):\n",
        "    \"\"\"Split text into manageable chunks\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_id2dBtuX3H"
      },
      "source": [
        "## 3. Knowledge Graph Construction\n",
        "\n",
        "Set up the graph transformer to extract entities and relationships:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KOEyQdD5uX3H"
      },
      "outputs": [],
      "source": [
        "def extract_knowledge_graph(text_chunks):\n",
        "    \"\"\"Extract knowledge graph from text chunks\"\"\"\n",
        "    # Initialize graph transformer with Gemini LLM\n",
        "    transformer = LLMGraphTransformer(\n",
        "        llm=llm,\n",
        "        node_properties=True,  # Extract node properties\n",
        "    )\n",
        "\n",
        "    # Process each chunk and accumulate results\n",
        "    all_triples = []\n",
        "\n",
        "    for i, chunk in enumerate(text_chunks):\n",
        "        print(f\"Processing chunk {i+1}/{len(text_chunks)}...\")\n",
        "        doc = Document(page_content=chunk)\n",
        "        # Convert text to graph triples\n",
        "        graph_documents = transformer.convert_to_graph_documents([doc])\n",
        "\n",
        "        # Extract triples\n",
        "        for graph_doc in graph_documents:\n",
        "            # Access the triples using the correct attribute name, which is 'relationships'\n",
        "            all_triples.extend([(r.source.id, r.type, r.target.id) for r in graph_doc.relationships])\n",
        "\n",
        "    return all_triples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ_qJwqHuX3I"
      },
      "source": [
        "Create functions to build and store the graph in NetworkX:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RUjmA7ytuX3I"
      },
      "outputs": [],
      "source": [
        "def build_networkx_graph(triples):\n",
        "    \"\"\"Build NetworkX graph from triples\"\"\"\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    for subj, pred, obj in triples:\n",
        "        # Add nodes if they don't exist\n",
        "        if not G.has_node(subj):\n",
        "            G.add_node(subj, label=subj)\n",
        "        if not G.has_node(obj):\n",
        "            G.add_node(obj, label=obj)\n",
        "\n",
        "        # Add edge with relationship as attribute\n",
        "        G.add_edge(subj, obj, label=pred)\n",
        "\n",
        "    return G\n",
        "\n",
        "def store_graph(graph, filename=\"knowledge_graph.graphml\"):\n",
        "    \"\"\"Store graph to GraphML file\"\"\"\n",
        "    nx.write_graphml(graph, filename)\n",
        "    print(f\"Graph saved to {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWAPSwyhuX3J"
      },
      "source": [
        "## 4. Personality Modeling\n",
        "\n",
        "Create functions to extract personality traits using Gemini and add them to the graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NZyewZLzuX3J"
      },
      "outputs": [],
      "source": [
        "def extract_personality_traits(text, person_name):\n",
        "    \"\"\"Extract personality traits for a given person using Gemini\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    From the following text, analyze the personality of {person_name}.\n",
        "    Return a JSON object with the following Big Five personality traits and scores between 0 and 1:\n",
        "    1. Openness (curiosity, creativity, openness to new experiences)\n",
        "    2. Conscientiousness (organization, responsibility, work ethic)\n",
        "    3. Extraversion (sociability, assertiveness, talkativeness)\n",
        "    4. Agreeableness (kindness, cooperation, empathy)\n",
        "    5. Neuroticism (anxiety, emotional instability, negative emotions)\n",
        "\n",
        "    Also include a list of 3-5 key personality descriptors (adjectives).\n",
        "\n",
        "    Format your response as valid JSON like this example:\n",
        "    {{\"openness\": 0.8, \"conscientiousness\": 0.7, \"extraversion\": 0.6, \"agreeableness\": 0.5, \"neuroticism\": 0.3, \"descriptors\": [\"creative\", \"organized\", \"friendly\"]}}\n",
        "\n",
        "    TEXT: {text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    content = response.content\n",
        "\n",
        "    # Extract JSON from response (handle potential formatting issues)\n",
        "    try:\n",
        "        # Try to extract JSON if it's embedded in text\n",
        "        start_idx = content.find(\"{\")\n",
        "        end_idx = content.rfind(\"}\") + 1\n",
        "        json_str = content[start_idx:end_idx]\n",
        "        traits = json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error parsing JSON for {person_name}. Using default values.\")\n",
        "        traits = {\n",
        "            \"openness\": 0.5,\n",
        "            \"conscientiousness\": 0.5,\n",
        "            \"extraversion\": 0.5,\n",
        "            \"agreeableness\": 0.5,\n",
        "            \"neuroticism\": 0.5,\n",
        "            \"descriptors\": [\"unknown\"]\n",
        "        }\n",
        "\n",
        "    # Ensure trait values are not None and are within the valid range [0, 1]\n",
        "    for trait in [\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]:\n",
        "        if trait not in traits or traits[trait] is None:\n",
        "            traits[trait] = 0.5  # Default to 0.5 if missing or None\n",
        "        else:\n",
        "            # Ensure the value is a float and clamp it between 0 and 1\n",
        "            try:\n",
        "                traits[trait] = float(traits[trait])\n",
        "                traits[trait] = max(0.0, min(1.0, traits[trait]))\n",
        "            except (ValueError, TypeError):\n",
        "                 traits[trait] = 0.5 # Default if conversion fails\n",
        "\n",
        "\n",
        "    return traits\n",
        "\n",
        "def identify_person_entities(graph):\n",
        "    \"\"\"Identify nodes that likely represent persons\"\"\"\n",
        "    # Use Gemini to identify which nodes are persons\n",
        "    persons = []\n",
        "    nodes = list(graph.nodes())\n",
        "\n",
        "    # Process in batches to avoid token limits\n",
        "    batch_size = 20\n",
        "    for i in range(0, len(nodes), batch_size):\n",
        "        batch = nodes[i:i+batch_size]\n",
        "        prompt = f\"\"\"\n",
        "        From the following list of entities, identify which ones are likely to be persons (people).\n",
        "        Return only the entities that represent people as a comma-separated list.\n",
        "\n",
        "        Entities: {', '.join(batch)}\n",
        "        \"\"\"\n",
        "\n",
        "        response = llm.invoke(prompt)\n",
        "        # Filter out empty strings and entities not present in the graph\n",
        "        batch_persons = [p.strip() for p in response.content.split(',') if p.strip() and p.strip() in nodes]\n",
        "        persons.extend(batch_persons)\n",
        "\n",
        "    # Remove duplicates\n",
        "    persons = list(set(persons))\n",
        "    return persons\n",
        "\n",
        "def add_personality_to_graph(graph, text, persons=None):\n",
        "    \"\"\"Add personality traits to person entities in the graph\"\"\"\n",
        "    if persons is None:\n",
        "        persons = identify_person_entities(graph)\n",
        "\n",
        "    print(f\"Adding personality traits for {len(persons)} identified persons...\")\n",
        "\n",
        "    for person in persons:\n",
        "        print(f\"Processing personality for: {person}\")\n",
        "        traits = extract_personality_traits(text, person)\n",
        "\n",
        "        # Add trait information as node attributes, ensuring values are not None\n",
        "        for trait, value in traits.items():\n",
        "            if trait != \"descriptors\" and value is not None:\n",
        "                graph.nodes[person][trait] = value\n",
        "\n",
        "        # Add descriptors, ensuring it's a string\n",
        "        descriptors = traits.get(\"descriptors\", [])\n",
        "        if isinstance(descriptors, list):\n",
        "             graph.nodes[person][\"descriptors\"] = \", \".join(descriptors)\n",
        "        else:\n",
        "             graph.nodes[person][\"descriptors\"] = str(descriptors) # Convert to string if not a list\n",
        "\n",
        "\n",
        "        # Optionally add trait nodes and relationships\n",
        "        for trait, value in traits.items():\n",
        "            if trait != \"descriptors\" and value is not None:\n",
        "                trait_node = f\"{trait.capitalize()}\"\n",
        "                if not graph.has_node(trait_node):\n",
        "                    graph.add_node(trait_node, label=trait_node)\n",
        "                # Store the value as both label property and weight property for edge\n",
        "                graph.add_edge(person, trait_node, label=f\"has_{trait}\", weight=value)\n",
        "\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnB27gsSuX3J"
      },
      "source": [
        "## 5. Graph Visualization\n",
        "\n",
        "Create functions to visualize the knowledge graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JfaYknofuX3J"
      },
      "outputs": [],
      "source": [
        "def visualize_graph(graph, output_file=\"knowledge_graph.html\"):\n",
        "    \"\"\"Visualize graph using PyVis\"\"\"\n",
        "    # Create PyVis network\n",
        "    net = Network(notebook=True, width=\"100%\", height=\"800px\", directed=True)\n",
        "\n",
        "    # Add nodes with labels and attributes\n",
        "    for node, attrs in graph.nodes(data=True):\n",
        "        # Prepare node attributes for visualization\n",
        "        title = f\"<b>{node}</b><br>\"\n",
        "\n",
        "        # Add personality traits to title if available\n",
        "        personality_traits = [\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
        "        for trait in personality_traits:\n",
        "            if trait in attrs:\n",
        "                title += f\"{trait.capitalize()}: {attrs[trait]:.2f}<br>\"\n",
        "\n",
        "        if \"descriptors\" in attrs:\n",
        "            title += f\"Descriptors: {attrs['descriptors']}\"\n",
        "\n",
        "        # Determine if node is a person (has personality traits)\n",
        "        is_person = any(trait in attrs for trait in personality_traits)\n",
        "\n",
        "        # Choose node color based on entity type\n",
        "        if is_person:\n",
        "            color = \"#ff6666\"  # Red for persons\n",
        "        elif any(trait == node for trait in [t.capitalize() for t in personality_traits]):\n",
        "            color = \"#66ff66\"  # Green for traits\n",
        "        else:\n",
        "            color = \"#6666ff\"  # Blue for other entities\n",
        "\n",
        "        # Add node to network\n",
        "        net.add_node(node, label=attrs.get(\"label\", node), title=title, color=color)\n",
        "\n",
        "    # Add edges with labels\n",
        "    for source, target, attrs in graph.edges(data=True):\n",
        "        edge_label = attrs.get(\"label\", \"\")\n",
        "        edge_weight = attrs.get(\"weight\", 1)\n",
        "\n",
        "        # Use the trait score as label instead of \"has_trait\"\n",
        "        if edge_label.startswith(\"has_\"):\n",
        "            # Use the weight (score) as the label\n",
        "            edge_label = f\"{edge_weight:.2f}\"\n",
        "\n",
        "        # Make personality trait edges thicker based on their value\n",
        "        width = 1\n",
        "        if attrs.get(\"label\", \"\").startswith(\"has_\"):\n",
        "            width = edge_weight * 5  # Scale up for visibility\n",
        "\n",
        "        net.add_edge(source, target, label=edge_label, width=width)\n",
        "\n",
        "    # Configure physics to allow nodes to be dragged and stay in place\n",
        "    net.set_options(\"\"\"\n",
        "    {\n",
        "      \"physics\": {\n",
        "        \"enabled\": false,\n",
        "        \"stabilization\": {\n",
        "          \"iterations\": 100,\n",
        "          \"fit\": true\n",
        "        },\n",
        "        \"barnesHut\": {\n",
        "          \"gravitationalConstant\": -2000,\n",
        "          \"centralGravity\": 0.1,\n",
        "          \"springLength\": 95,\n",
        "          \"springConstant\": 0.04\n",
        "        },\n",
        "        \"solver\": \"barnesHut\"\n",
        "      },\n",
        "      \"interaction\": {\n",
        "        \"dragNodes\": true,\n",
        "        \"navigationButtons\": true\n",
        "      },\n",
        "      \"layout\": {\n",
        "        \"improvedLayout\": true\n",
        "      }\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "    # Save and display\n",
        "    net.save_graph(output_file)\n",
        "    return display(HTML(f\"<a href='{output_file}' target='_blank'>Open visualization in new tab</a>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN3ydC4nuX3K"
      },
      "source": [
        "## 6. Graph Querying\n",
        "\n",
        "Create functions to query the knowledge graph:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4Pp7NiLjuX3K"
      },
      "outputs": [],
      "source": [
        "def query_graph_llm(graph, query_text):\n",
        "    \"\"\"Query the knowledge graph using natural language and Gemini\"\"\"\n",
        "    # Collect node information including personality traits\n",
        "    node_info = []\n",
        "    for node, attrs in graph.nodes(data=True):\n",
        "        # Get basic node information\n",
        "        info = f\"Node: {node}\"\n",
        "\n",
        "        # Add personality traits if they exist\n",
        "        personality_traits = [\"openness\", \"conscientiousness\", \"extraversion\", \"agreeableness\", \"neuroticism\"]\n",
        "        trait_info = []\n",
        "        for trait in personality_traits:\n",
        "            if trait in attrs:\n",
        "                trait_info.append(f\"{trait}={attrs[trait]:.2f}\")\n",
        "\n",
        "        if trait_info:\n",
        "            info += f\" [Traits: {', '.join(trait_info)}]\"\n",
        "\n",
        "        # Add descriptors if they exist\n",
        "        if \"descriptors\" in attrs:\n",
        "            info += f\" [Descriptors: {attrs['descriptors']}]\"\n",
        "\n",
        "        node_info.append(info)\n",
        "\n",
        "    # Collect edge information\n",
        "    edge_info = []\n",
        "    for src, tgt, attrs in graph.edges(data=True):\n",
        "        edge_label = attrs.get('label', 'related_to')\n",
        "        edge_weight = attrs.get('weight', None)\n",
        "\n",
        "        if edge_weight is not None:\n",
        "            edge_info.append(f\"({src})-[{edge_label} {edge_weight:.2f}]->({tgt})\")\n",
        "        else:\n",
        "            edge_info.append(f\"({src})-[{edge_label}]->({tgt})\")\n",
        "\n",
        "    # Create prompt with detailed graph information\n",
        "    prompt = f\"\"\"\n",
        "    You are given a knowledge graph with the following nodes and relationships.\n",
        "    Answer the question based on this graph information.\n",
        "\n",
        "    NODES (including personality traits when available):\n",
        "    {chr(10).join(node_info)}\n",
        "\n",
        "    RELATIONSHIPS:\n",
        "    {chr(10).join(edge_info)}\n",
        "\n",
        "    Question: {query_text}\n",
        "\n",
        "    When answering questions about personality traits, use the numeric values to make comparisons.\n",
        "    For example, if asked who is most extraverted, compare the extraversion scores.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "def find_entities_by_trait(graph, trait, threshold=0.7):\n",
        "    \"\"\"Find entities with a high score for a specific personality trait\"\"\"\n",
        "    results = []\n",
        "    for node, attrs in graph.nodes(data=True):\n",
        "        if trait.lower() in attrs and attrs[trait.lower()] >= threshold:\n",
        "            results.append((node, attrs[trait.lower()]))\n",
        "\n",
        "    # Sort by trait value (descending)\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWNouktvuX3K"
      },
      "source": [
        "## 7. Complete Pipeline\n",
        "\n",
        "Let's put everything together in a complete pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u5sDMVqkuX3K"
      },
      "outputs": [],
      "source": [
        "def build_knowledge_graph_with_personality(text_or_file, is_file=False):\n",
        "    \"\"\"Build a knowledge graph with personality modeling from text\"\"\"\n",
        "    # Load document if it's a file\n",
        "    if is_file:\n",
        "        text = load_document(text_or_file)\n",
        "        # Extract filename without path or extension for output file naming\n",
        "        import os\n",
        "        base_filename = os.path.splitext(os.path.basename(text_or_file))[0]\n",
        "        graph_filename = f\"{base_filename}_graph.graphml\"\n",
        "        html_filename = f\"{base_filename}_graph.html\"\n",
        "    else:\n",
        "        text = text_or_file\n",
        "        graph_filename = \"knowledge_graph.graphml\"\n",
        "        html_filename = \"knowledge_graph.html\"\n",
        "\n",
        "    print(\"1. Preprocessing text...\")\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    print(\"2. Chunking text...\")\n",
        "    chunks = chunk_text(processed_text)\n",
        "    print(f\"   Created {len(chunks)} chunks\")\n",
        "\n",
        "    print(\"3. Extracting knowledge graph...\")\n",
        "    triples = extract_knowledge_graph(chunks)\n",
        "    print(f\"   Extracted {len(triples)} triples\")\n",
        "\n",
        "    print(\"4. Building graph...\")\n",
        "    graph = build_networkx_graph(triples)\n",
        "    print(f\"   Graph has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges\")\n",
        "\n",
        "    print(\"5. Adding personality traits...\")\n",
        "    graph = add_personality_to_graph(graph, processed_text)\n",
        "\n",
        "    print(\"6. Saving and visualizing graph...\")\n",
        "    store_graph(graph, graph_filename)\n",
        "    visualize_graph(graph, html_filename)\n",
        "\n",
        "    return graph, processed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-fLjZWouX3K"
      },
      "source": [
        "## 8. Example Usage\n",
        "\n",
        "Let's test our pipeline with a sample text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "6db4ISv1uX3L",
        "outputId": "4b7dc888-f191-48a9-f0bc-ed4567dd7dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Preprocessing text...\n",
            "2. Chunking text...\n",
            "   Created 1 chunks\n",
            "3. Extracting knowledge graph...\n",
            "Processing chunk 1/1...\n",
            "   Extracted 52 triples\n",
            "4. Building graph...\n",
            "   Graph has 44 nodes and 46 edges\n",
            "5. Adding personality traits...\n",
            "Adding personality traits for 4 identified persons...\n",
            "Processing personality for: Albert Einstein\n",
            "Processing personality for: Franklin D. Roosevelt\n",
            "Processing personality for: Adolf Hitler\n",
            "Processing personality for: Satyendra Nath Bose\n",
            "6. Saving and visualizing graph...\n",
            "Graph saved to test_graph.graphml\n",
            "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<a href='test_graph.html' target='_blank'>Open visualization in new tab</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sample_text = \"\"\"\n",
        "John Smith is the CEO of TechInnovate. He's known for his visionary thinking and creative problem-solving approach.\n",
        "John often works closely with Sarah Johnson, the CTO, who is highly analytical and detail-oriented.\n",
        "Sarah joined the company in 2018 after leaving her role at DataSystems.\n",
        "\n",
        "Michael Brown, TechInnovate's Head of Marketing, is outgoing and charismatic. He leads a team of 15 people\n",
        "and reports directly to John. Michael previously worked at MediaCorp for 5 years.\n",
        "\n",
        "TechInnovate was founded in 2015 and is headquartered in San Francisco. The company specializes in AI solutions\n",
        "for healthcare and has partnerships with several major hospitals, including Metropolitan Hospital.\n",
        "\n",
        "Dr. Emily Chen is the Chief Medical Officer at Metropolitan Hospital. She is calm, patient, and meticulous in her work.\n",
        "Emily has been collaborating with TechInnovate on their latest AI diagnostic tool.\n",
        "\"\"\"\n",
        "\n",
        "sample_file = 'test.txt'\n",
        "\n",
        "# Run the pipeline\n",
        "graph, text = build_knowledge_graph_with_personality(sample_file, is_file = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5fYfMAWuX3L"
      },
      "source": [
        "Now let's query our graph to demonstrate some insights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWE2E4CSuX3L"
      },
      "outputs": [],
      "source": [
        "# Find people with high openness scores\n",
        "open_people = find_entities_by_trait(graph, \"openness\", 0.7)\n",
        "print(\"People with high openness scores:\")\n",
        "for person, score in open_people:\n",
        "    print(f\"- {person}: {score:.2f}\")\n",
        "\n",
        "# Find relationships between key people\n",
        "query_result = query_graph_llm(graph, \"What is the relationship between John Smith and Sarah Johnson?\")\n",
        "print(\"\\nRelationship query result:\")\n",
        "print(query_result)\n",
        "\n",
        "# Find personality insights\n",
        "query_result = query_graph_llm(graph, \"Who is the most extroverted person in the graph and what is their role?\")\n",
        "print(\"\\nPersonality insight query result:\")\n",
        "print(query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpphB0JVuX3M"
      },
      "source": [
        "## 9. Conclusion\n",
        "\n",
        "This notebook demonstrates a complete pipeline for:\n",
        "1. Extracting a knowledge graph from unstructured text using LangChain and Gemini\n",
        "2. Enriching the graph with personality traits\n",
        "3. Visualizing and querying the resulting knowledge graph\n",
        "\n",
        "The approach can be extended with:\n",
        "- More sophisticated entity resolution\n",
        "- Integration with external knowledge bases\n",
        "- Implementing graph-based RAG for enhanced question answering\n",
        "- Fine-tuning personality modeling for domain-specific applications"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
